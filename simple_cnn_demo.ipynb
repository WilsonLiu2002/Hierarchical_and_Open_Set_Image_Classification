{
 "cells": [
  {
   "cell_type": "code",
   "id": "198c699a-e1e8-4f8b-8cd5-98a1d05f7ec3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T00:08:32.777270Z",
     "start_time": "2025-05-11T00:08:32.773541Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import clip\n",
    "\n",
    "import torchvision\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, BatchSampler, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ],
   "outputs": [],
   "execution_count": 74
  },
  {
   "cell_type": "code",
   "id": "c370d643-46fd-4d03-bb17-a875e79d5e2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T00:08:32.786427Z",
     "start_time": "2025-05-11T00:08:32.781514Z"
    }
   },
   "source": [
    "# Create Dataset class for multilabel classification\n",
    "class CLIPFineTuneDataset(Dataset):\n",
    "    def __init__(self, ann_df, super_map_df, sub_map_df, img_dir, transform=None):\n",
    "        self.ann_df = ann_df \n",
    "        self.super_map_df = super_map_df\n",
    "        self.sub_map_df = sub_map_df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ann_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.ann_df['image'][idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        super_idx = self.ann_df['superclass_index'][idx]\n",
    "        super_label = self.super_map_df['class'][super_idx]\n",
    "\n",
    "        sub_idx = self.ann_df['subclass_index'][idx]\n",
    "        sub_label = self.sub_map_df['class'][sub_idx]\n",
    "\n",
    "        description = self.ann_df['description'][idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, super_idx, super_label, sub_idx, sub_label, description\n",
    "\n",
    "class MultiClassImageTestDataset(Dataset):\n",
    "    def __init__(self, super_map_df, sub_map_df, img_dir, transform=None):\n",
    "        self.super_map_df = super_map_df\n",
    "        self.sub_map_df = sub_map_df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self): # Count files in img_dir\n",
    "        return len([fname for fname in os.listdir(self.img_dir)])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = str(idx) + '.jpg'\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)  \n",
    "            \n",
    "        return image, img_name"
   ],
   "outputs": [],
   "execution_count": 75
  },
  {
   "cell_type": "code",
   "id": "e7398553-8842-4ad8-b348-767921a22482",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T00:08:32.823923Z",
     "start_time": "2025-05-11T00:08:32.808039Z"
    }
   },
   "source": [
    "train_ann_df = pd.read_csv('data/train_data.csv')\n",
    "test_ann_df = pd.read_csv('data/example_test_predictions.csv')\n",
    "super_map_df = pd.read_csv('data/superclass_mapping.csv')\n",
    "sub_map_df = pd.read_csv('data/subclass_mapping.csv')\n",
    "\n",
    "train_img_dir = 'data/train_images'\n",
    "test_img_dir = 'data/test_images'\n",
    "\n",
    "image_preprocessing = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to match CLIP expectations\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "                         std=(0.26862954, 0.26130258, 0.27577711))  # CLIP's normalization\n",
    "])\n",
    "\n",
    "# Create train and val split\n",
    "train_dataset = CLIPFineTuneDataset(train_ann_df, super_map_df, sub_map_df, train_img_dir, transform=image_preprocessing)\n",
    "train_dataset, val_dataset = random_split(train_dataset, [0.9, 0.1]) \n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = MultiClassImageTestDataset(super_map_df, sub_map_df, test_img_dir, transform=image_preprocessing)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                         batch_size=1, \n",
    "                         shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 76
  },
  {
   "cell_type": "code",
   "id": "bf33a131-0c66-40dc-b8d4-ba5d0f840840",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T00:08:32.849228Z",
     "start_time": "2025-05-11T00:08:32.841289Z"
    }
   },
   "source": [
    "class CLIPClassifierTrainer():\n",
    "    def __init__(self, clip_model, criterion, optimizer, train_loader, val_loader, test_loader=None, device=None):\n",
    "        self.clip_model = clip_model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.device = device or (torch.device('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "\n",
    "        for p in self.clip_model.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        self.super_head = nn.Linear(512, 4).to(self.device)\n",
    "        self.sub_head = nn.Linear(512, 88).to(self.device)\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.super_head.train()\n",
    "        self.sub_head.train()\n",
    "        self.clip_model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(self.train_loader):\n",
    "            inputs, super_labels, sub_labels, texts = data[0].to(self.device), data[1].to(self.device), data[3].to(self.device), data[5]\n",
    "\n",
    "            text_tokens = clip.tokenize(list(texts)).to(self.device)\n",
    "\n",
    "            image_features = self.clip_model.encode_image(inputs)\n",
    "            text_features = self.clip_model.encode_text(text_tokens)\n",
    "\n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            assert image_features.size(0) == text_features.size(0), \"Batch size mismatch between images and texts\"\n",
    "\n",
    "            logits_per_image = 100.0 * image_features @ text_features.T\n",
    "            logits_per_text = logits_per_image.T\n",
    "\n",
    "            ground_truth = torch.arange(len(inputs), device=self.device)\n",
    "            contrastive_loss = (self.criterion(logits_per_image, ground_truth) + self.criterion(logits_per_text, ground_truth)) / 2\n",
    "\n",
    "            super_outputs = self.super_head(image_features)\n",
    "            sub_outputs = self.sub_head(image_features)\n",
    "            classification_loss = F.cross_entropy(super_outputs, super_labels) + F.cross_entropy(sub_outputs, sub_labels)\n",
    "\n",
    "            total_loss = contrastive_loss + classification_loss\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            running_loss += total_loss.item()\n",
    "\n",
    "        print(f'Training loss: {running_loss / (i + 1):.3f}')\n",
    "\n",
    "    def validate_epoch(self):\n",
    "        self.super_head.eval()\n",
    "        self.sub_head.eval()\n",
    "        self.clip_model.eval()\n",
    "\n",
    "        super_correct = 0\n",
    "        sub_correct = 0\n",
    "        total = 0\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(self.val_loader):\n",
    "                inputs, super_labels, sub_labels, texts = data[0].to(self.device), data[1].to(self.device), data[3].to(self.device), data[5]\n",
    "\n",
    "                text_tokens = clip.tokenize(list(texts)).squeeze(1).to(self.device)\n",
    "\n",
    "                image_features = self.clip_model.encode_image(inputs)\n",
    "                text_features = self.clip_model.encode_text(text_tokens)\n",
    "\n",
    "                image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "                text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "                assert image_features.size(0) == text_features.size(0), \"Batch size mismatch between images and texts\"\n",
    "\n",
    "                logits_per_image = 100.0 * image_features @ text_features.T\n",
    "                logits_per_text = logits_per_image.T\n",
    "\n",
    "                ground_truth = torch.arange(len(inputs), device=self.device)\n",
    "                contrastive_loss = (self.criterion(logits_per_image, ground_truth) + self.criterion(logits_per_text, ground_truth)) / 2\n",
    "\n",
    "                super_outputs = self.super_head(image_features)\n",
    "                sub_outputs = self.sub_head(image_features)\n",
    "                classification_loss = F.cross_entropy(super_outputs, super_labels) + F.cross_entropy(sub_outputs, sub_labels)\n",
    "\n",
    "                total_loss = contrastive_loss + classification_loss\n",
    "                running_loss += total_loss.item()\n",
    "\n",
    "                _, super_predicted = torch.max(super_outputs.data, 1)\n",
    "                _, sub_predicted = torch.max(sub_outputs.data, 1)\n",
    "\n",
    "                total += super_labels.size(0)\n",
    "                super_correct += (super_predicted == super_labels).sum().item()\n",
    "                sub_correct += (sub_predicted == sub_labels).sum().item()\n",
    "\n",
    "        print(f'Validation loss: {running_loss / (i + 1):.3f}')\n",
    "        print(f'Validation superclass acc: {100 * super_correct / total:.2f} %')\n",
    "        print(f'Validation subclass acc: {100 * sub_correct / total:.2f} %')\n",
    "\n",
    "    def test(self, save_to_csv=False, return_predictions=False):\n",
    "        if not self.test_loader:\n",
    "            raise NotImplementedError('test_loader not specified')\n",
    "\n",
    "        test_predictions = {'image': [], 'superclass_index': [], 'subclass_index': []}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(self.test_loader):\n",
    "                inputs, img_name = data[0].to(self.device), data[1]\n",
    "                image_features = self.clip_model.encode_image(inputs)\n",
    "                super_outputs = self.super_head(image_features)\n",
    "                sub_outputs = self.sub_head(image_features)\n",
    "\n",
    "                _, super_predicted = torch.max(super_outputs.data, 1)\n",
    "                _, sub_predicted = torch.max(sub_outputs.data, 1)\n",
    "\n",
    "                test_predictions['image'].append(img_name[0])\n",
    "                test_predictions['superclass_index'].append(super_predicted.item())\n",
    "                test_predictions['subclass_index'].append(sub_predicted.item())\n",
    "\n",
    "        test_predictions = pd.DataFrame(data=test_predictions)\n",
    "\n",
    "        if save_to_csv:\n",
    "            test_predictions.to_csv('example_test_predictions.csv', index=False)\n",
    "\n",
    "        if return_predictions:\n",
    "            return test_predictions"
   ],
   "outputs": [],
   "execution_count": 77
  },
  {
   "cell_type": "code",
   "id": "ebdf524a-98bf-4d0b-9b63-2b2b7b87daa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T00:08:34.885111Z",
     "start_time": "2025-05-11T00:08:32.883844Z"
    }
   },
   "source": [
    "# Init model and trainer\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "super_head = nn.Linear(512, 4).to(device)\n",
    "sub_head = nn.Linear(512, 88).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(\n",
    "    list(clip_model.parameters()) + list(super_head.parameters()) + list(sub_head.parameters()), lr=1e-5\n",
    ")\n",
    "trainer = CLIPClassifierTrainer(clip_model, criterion, optimizer, train_loader, val_loader, test_loader)\n",
    "trainer.super_head = super_head\n",
    "trainer.sub_head = sub_head"
   ],
   "outputs": [],
   "execution_count": 78
  },
  {
   "cell_type": "code",
   "id": "7941c289-d9b1-4714-b788-898b3b889f58",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-11T00:08:34.889498Z"
    }
   },
   "source": [
    "# Training loop\n",
    "for epoch in range(20):\n",
    "    print(f'Epoch {epoch+1}')\n",
    "    trainer.train_epoch()\n",
    "    trainer.validate_epoch()\n",
    "    print('')\n",
    "\n",
    "print('Finished Training')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d17e37-1a08-4ae1-8517-a16ff4769622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab70fb9-6e14-49f1-b9bb-5f3da6807399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698f0ba0-08ef-4093-a154-65d735174a25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d43947-4569-4920-929b-c997be9def58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61d77cbe-3ba1-46e9-9be1-208b9cabab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = trainer.test(save_to_csv=True, return_predictions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f4b681a-9e26-41f9-b0d1-4f1bc4077edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Superclass Accuracy\n",
      "Overall: 43.83 %\n",
      "Seen: 61.11 %\n",
      "Unseen: 0.00 %\n",
      "\n",
      "Subclass Accuracy\n",
      "Overall: 2.03 %\n",
      "Seen: 9.56 %\n",
      "Unseen: 0.00 %\n"
     ]
    }
   ],
   "source": [
    "# Quick script for evaluating generated csv files with ground truth\n",
    "\n",
    "super_correct = 0\n",
    "sub_correct = 0\n",
    "seen_super_correct = 0\n",
    "seen_sub_correct = 0\n",
    "unseen_super_correct = 0\n",
    "unseen_sub_correct = 0\n",
    "\n",
    "total = 0\n",
    "seen_super_total = 0\n",
    "unseen_super_total = 0\n",
    "seen_sub_total = 0\n",
    "unseen_sub_total = 0\n",
    "\n",
    "for i in range(len(test_predictions)):\n",
    "    super_pred = test_predictions['superclass_index'][i]\n",
    "    sub_pred = test_predictions['subclass_index'][i]\n",
    "\n",
    "    super_gt = test_ann_df['superclass_index'][i]\n",
    "    sub_gt = test_ann_df['subclass_index'][i]\n",
    "\n",
    "    # Total setting\n",
    "    if super_pred == super_gt:\n",
    "        super_correct += 1\n",
    "    if sub_pred == sub_gt:\n",
    "        sub_correct += 1\n",
    "    total += 1\n",
    "\n",
    "    # Unseen superclass setting\n",
    "    if super_gt == 3:\n",
    "        if super_pred == super_gt:\n",
    "            unseen_super_correct += 1\n",
    "        if sub_pred == sub_gt:\n",
    "            unseen_sub_correct += 1\n",
    "        unseen_super_total += 1\n",
    "        unseen_sub_total += 1\n",
    "    \n",
    "    # Seen superclass, unseen subclass setting\n",
    "    if super_gt != 3 and sub_gt == 87:\n",
    "        if super_pred == super_gt:\n",
    "            seen_super_correct += 1\n",
    "        if sub_pred == sub_gt:\n",
    "            unseen_sub_correct += 1\n",
    "        seen_super_total += 1\n",
    "        unseen_sub_total += 1\n",
    "\n",
    "    # Seen superclass and subclass setting\n",
    "    if super_gt != 3 and sub_gt != 87:\n",
    "        if super_pred == super_gt:\n",
    "            seen_super_correct += 1\n",
    "        if sub_pred == sub_gt:\n",
    "            seen_sub_correct += 1\n",
    "        seen_super_total += 1\n",
    "        seen_sub_total += 1\n",
    "\n",
    "print('Superclass Accuracy')\n",
    "print(f'Overall: {100*super_correct/total:.2f} %')\n",
    "print(f'Seen: {100*seen_super_correct/seen_super_total:.2f} %')\n",
    "print(f'Unseen: {100*unseen_super_correct/unseen_super_total:.2f} %')\n",
    "\n",
    "print('\\nSubclass Accuracy')\n",
    "print(f'Overall: {100*sub_correct/total:.2f} %')\n",
    "print(f'Seen: {100*seen_sub_correct/seen_sub_total:.2f} %')\n",
    "print(f'Unseen: {100*unseen_sub_correct/unseen_sub_total:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed6dd55-94ab-41a9-b09c-08d7b1ae0fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
