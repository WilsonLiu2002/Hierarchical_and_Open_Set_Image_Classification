{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5_PLXfjpsf_m"
      },
      "outputs": [],
      "source": [
        "import os, random, copy, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet50, ResNet50_Weights"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.  Dataset definitions\n",
        "class MultiClassImageDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, transform):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img = Image.open(os.path.join(self.img_dir, row['image'])).convert('RGB')\n",
        "        if self.transform: img = self.transform(img)\n",
        "        return img, row['superclass_index'], row['subclass_index']\n",
        "\n",
        "class ImageOnlyDataset(Dataset):\n",
        "    def __init__(self, img_dir, transform):\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.names = sorted(os.listdir(img_dir))\n",
        "\n",
        "    def __len__(self): return len(self.names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(os.path.join(self.img_dir, self.names[idx])).convert('RGB')\n",
        "        if self.transform: img = self.transform(img)\n",
        "        return img, self.names[idx]"
      ],
      "metadata": {
        "id": "8LTXCbnhsvWe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.  Data split with held-out novel sub-classes\n",
        "FULL_CSV  = 'train_data.csv'\n",
        "IMG_DIR   = 'train_shuffle'\n",
        "TEST_DIR  = 'test_shuffle'\n",
        "BATCH_SZ  = 64\n",
        "NUM_WORK  = 4\n",
        "\n",
        "full_df = pd.read_csv(FULL_CSV)\n",
        "\n",
        "# pick 5 sub-classes to simulate novelty\n",
        "novel_ids = [7, 9, 14, 18, 43]\n",
        "val_novel_df = full_df[full_df['subclass_index'].isin(novel_ids)]\n",
        "seen_df      = full_df[~full_df['subclass_index'].isin(novel_ids)]\n",
        "\n",
        "train_df, val_df = train_test_split(\n",
        "    seen_df, test_size=0.10, stratify=seen_df['superclass_index'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f'Train {len(train_df)}, Val {len(val_df)}, Val-Novel {len(val_novel_df)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_ui7y_wsznN",
        "outputId": "437cf13a-3e88-492b-9502-c1d9bc8b59e3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train 5300, Val 589, Val-Novel 399\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.  Transforms  +  MixUp helper\n",
        "IM_MU, IM_SIG = (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)\n",
        "\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.7,1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(0.25,0.25,0.25,0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IM_MU, IM_SIG),\n",
        "])\n",
        "\n",
        "test_tf = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IM_MU, IM_SIG),\n",
        "])\n",
        "\n",
        "def mixup_data(x, y1, y2, alpha=1.0):\n",
        "    if alpha <= 0.0: return x, y1, y2, 1.0\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size, device=x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index]\n",
        "    y1_a, y1_b = y1, y1[index]\n",
        "    y2_a, y2_b = y2, y2[index]\n",
        "    return mixed_x, (y1_a, y1_b, y2_a, y2_b), lam\n",
        "\n",
        "def mixup_criterion(crit, pred, targets, lam):\n",
        "    sup_out, sub_out = pred\n",
        "    y1a,y1b,y2a,y2b = targets\n",
        "    return lam * (crit(sup_out, y1a)+crit(sub_out, y2a)) + \\\n",
        "           (1-lam)*(crit(sup_out, y1b)+crit(sub_out, y2b))"
      ],
      "metadata": {
        "id": "skb0qWPFs1n_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.  Model\n",
        "class ResNet50Multi(nn.Module):\n",
        "    def __init__(self, n_super=3, n_sub=88, pretrained=True):\n",
        "        super().__init__()\n",
        "        weights = ResNet50_Weights.IMAGENET1K_V2 if pretrained else None\n",
        "        self.backbone = resnet50(weights=weights)\n",
        "        in_feats = self.backbone.fc.in_features\n",
        "        self.backbone.fc = nn.Identity()\n",
        "        self.shared = nn.Sequential(nn.Linear(in_feats,512), nn.ReLU())\n",
        "        self.super_head = nn.Linear(512, n_super)\n",
        "        self.sub_head   = nn.Linear(512, n_sub)\n",
        "\n",
        "    def forward(self, x, return_feat=False):\n",
        "        feat = self.shared(self.backbone(x))\n",
        "        if return_feat: return feat\n",
        "        return self.super_head(feat), self.sub_head(feat)"
      ],
      "metadata": {
        "id": "2BZ-AY6utGtl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip -q test_shuffle.zip\n",
        "# !unzip -q train_shuffle.zip"
      ],
      "metadata": {
        "id": "M25s1X8ds66l"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.  Dataloaders\n",
        "train_ds = MultiClassImageDataset(train_df, IMG_DIR, train_tf)\n",
        "val_ds   = MultiClassImageDataset(val_df,   IMG_DIR, test_tf)\n",
        "novel_ds = MultiClassImageDataset(val_novel_df, IMG_DIR, test_tf)\n",
        "test_ds  = ImageOnlyDataset(TEST_DIR, test_tf)\n",
        "\n",
        "train_ld = DataLoader(train_ds,  BATCH_SZ, shuffle=True,  num_workers=NUM_WORK)\n",
        "val_ld   = DataLoader(val_ds,    BATCH_SZ, shuffle=False, num_workers=NUM_WORK)\n",
        "novel_ld = DataLoader(novel_ds,  BATCH_SZ, shuffle=False, num_workers=NUM_WORK)\n",
        "test_ld  = DataLoader(test_ds,   1,        shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e2nGeNns9_W",
        "outputId": "d2157b75-bfb2-425e-d0fb-f260963ba92f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.  Training loop with two phases\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = ResNet50Multi().to(device)\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "for p in model.backbone.parameters():\n",
        "    if not p.requires_grad: continue\n",
        "    p.requires_grad = False\n",
        "for n, p in model.backbone.named_parameters():\n",
        "    if n.startswith('layer4'): p.requires_grad = True\n",
        "\n",
        "opt = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "def run_epoch(phase):\n",
        "    is_train = phase=='train'\n",
        "    model.train(is_train)\n",
        "    loader = train_ld if is_train else val_ld\n",
        "    tot, correct_s, correct_sub, loss_sum = 0,0,0,0\n",
        "    for x,sup,sub in loader:\n",
        "        x,sup,sub = x.to(device),sup.to(device),sub.to(device)\n",
        "        if is_train:\n",
        "            x, targets, lam = mixup_data(x,sup,sub,alpha=0.4)\n",
        "            opt.zero_grad()\n",
        "            out = model(x)\n",
        "            loss = mixup_criterion(criterion,out,targets,lam)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "        else:\n",
        "            sup_out, sub_out = model(x)\n",
        "            loss = criterion(sup_out,sup)+criterion(sub_out,sub)\n",
        "        loss_sum += loss.item()\n",
        "        with torch.no_grad():\n",
        "            sup_p = model(x)[0].argmax(1)\n",
        "            sub_p = model(x)[1].argmax(1)\n",
        "            correct_s += (sup_p==sup).sum().item()\n",
        "            correct_sub += (sub_p==sub).sum().item()\n",
        "            tot += sup.size(0)\n",
        "    return loss_sum/len(loader), 100*correct_s/tot, 100*correct_sub/tot\n",
        "\n",
        "print('Phase 1: train heads + layer4 (5 epochs)')\n",
        "for ep in range(5):\n",
        "    tr = run_epoch('train')\n",
        "    vl = run_epoch('val')\n",
        "    print(f'E{ep+1}: TrainLoss {tr[0]:.3f} | ValSup {vl[1]:.2f}%  ValSub {vl[2]:.2f}%')\n",
        "\n",
        "# full-fine-tune phase\n",
        "for p in model.parameters(): p.requires_grad = True\n",
        "opt = optim.AdamW(model.parameters(), lr=3e-5, weight_decay=1e-4)\n",
        "sched = CosineAnnealingLR(opt, T_max=15)\n",
        "\n",
        "print('Phase 2: full fine-tune + cosine LR (15 epochs)')\n",
        "best_loss, best_state = 1e9, None\n",
        "for ep in range(15):\n",
        "    tr = run_epoch('train')\n",
        "    vl = run_epoch('val')\n",
        "    sched.step()\n",
        "    print(f'E{ep+6}: TrainLoss {tr[0]:.3f} | ValSup {vl[1]:.2f}%  ValSub {vl[2]:.2f}%')\n",
        "    if vl[0] < best_loss:\n",
        "        best_loss, best_state = vl[0], copy.deepcopy(model.state_dict())\n",
        "\n",
        "model.load_state_dict(best_state)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "tuz_dA7GtCaC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7.  Build prototypes & choose novelty threshold θ\n",
        "print('Building prototypes and calibrating θ')\n",
        "proto = {}\n",
        "with torch.no_grad():\n",
        "    for x,_,sub in DataLoader(train_ds, BATCH_SZ):\n",
        "        feat = model(x.to(device), return_feat=True).cpu()\n",
        "        for f, idx in zip(feat, sub):\n",
        "            idx=int(idx); proto.setdefault(idx,[]).append(f)\n",
        "proto = {k: torch.stack(v).mean(0) for k,v in proto.items()}\n",
        "\n",
        "def min_cos_dist(feat):\n",
        "    \"\"\"feat: (B,512) → (B,) minimum cosine distance to any prototype\"\"\"\n",
        "    dists = [1-F.cosine_similarity(feat, p.to(feat.device), dim=1)\n",
        "             for p in proto.values()]\n",
        "    return torch.stack(dists).min(0)[0]\n",
        "\n",
        "# collect distances on seen and novel validation sets\n",
        "seen_d, novel_d = [],[]\n",
        "with torch.no_grad():\n",
        "    for ld,bucket in [(val_ld, seen_d), (novel_ld, novel_d)]:\n",
        "        for x,_,_ in ld:\n",
        "            d = min_cos_dist(model(x.to(device),return_feat=True))\n",
        "            bucket.extend(d.cpu())\n",
        "\n",
        "seen_d, novel_d = np.array(seen_d), np.array(novel_d)\n",
        "# theta = 0.5\n",
        "theta = np.percentile(seen_d, 99)\n",
        "print(f'Chosen θ = {theta:.3f}   (median novel distance = {np.median(novel_d):.3f})')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2N76JGdtESM",
        "outputId": "f9c13c35-2aa0-4b36-e61e-bc11c5e036b0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Building prototypes and calibrating θ …\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chosen θ = 0.500   (median novel distance = 0.056)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8.  Inference on hidden test set\n",
        "print('Running inference on test set …')\n",
        "# theta = 0.99\n",
        "rows=[]\n",
        "with torch.no_grad():\n",
        "    for x,name in test_ld:\n",
        "        x=x.to(device)\n",
        "        feat = model(x, return_feat=True)\n",
        "        sup_out, sub_out = model(x)\n",
        "        sup_idx = sup_out.argmax(1).item()\n",
        "        sub_idx = sub_out.argmax(1).item()\n",
        "        if min_cos_dist(feat).item() > theta:\n",
        "            sub_idx = 87\n",
        "        rows.append((name, sup_idx, sub_idx))\n",
        "\n",
        "subm = pd.DataFrame(rows, columns=['image','superclass_index','subclass_index'])\n",
        "subm.to_csv('submission.csv', index=False)\n",
        "print('Saved submission.csv with', len(subm), 'rows.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awkblQ4vtT84",
        "outputId": "ecd0909a-4915-4bb7-cda7-888ee9b89490"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Running inference on test set …\n",
            "Saved submission.csv with 11180 rows.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zeej87Wp0Kz6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}